{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-29T00:34:25.072535Z",
     "start_time": "2024-01-29T00:34:25.050782Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, precision_recall_fscore_support, roc_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tqdm import tqdm\n",
    "from typing import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ag_news (/Users/jaypark/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94bd80d089fb45a696467fee060159c8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"ag_news\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T00:34:28.262267Z",
     "start_time": "2024-01-29T00:34:25.056120Z"
    }
   },
   "id": "51bf161405e1a296",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # 모델 입력에 사용되는 데이터를 바로 사용할 수 없고 토큰화를 해서 입력 그떄 사용하는 토크나이저\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4) \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T00:34:29.701280Z",
     "start_time": "2024-01-29T00:34:28.263139Z"
    }
   },
   "id": "90a5b5a8b710ea31",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DatasetItem(TypedDict):\n",
    "    text: str\n",
    "    label: str\n",
    "    \n",
    "def preprocess_date(dataset_item: DatasetItem)->dict[str, torch.Tensor]:\n",
    "    return tokenizer(dataset_item[\"text\"], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T00:34:29.706361Z",
     "start_time": "2024-01-29T00:34:29.703660Z"
    }
   },
   "id": "dd915b692e521ddf",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/jaypark/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-6aefbb7bb7939cd6.arrow\n",
      "Loading cached processed dataset at /Users/jaypark/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-59c08fe492775d92.arrow\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리\n",
    "\n",
    "train_dataset = dataset[\"train\"].select(range(1200)).map(preprocess_date, batched=True)\n",
    "test_dataset = dataset[\"test\"].select(range(1200)).map(preprocess_date, batched=True)\n",
    "\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\",\"label\"])\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\",\"label\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T00:34:29.916899Z",
     "start_time": "2024-01-29T00:34:29.707491Z"
    }
   },
   "id": "c15d1c39fa0fcd58",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   text  label  \\\n0     Wall St. Bears Claw Back Into the Black (Reute...      2   \n1     Carlyle Looks Toward Commercial Aerospace (Reu...      2   \n2     Oil and Economy Cloud Stocks' Outlook (Reuters...      2   \n3     Iraq Halts Oil Exports from Main Southern Pipe...      2   \n4     Oil prices soar to all-time record, posing new...      2   \n...                                                 ...    ...   \n1195  Airline Looks to Delay Its Pension Payments In...      2   \n1196  McDonald's Goes for Gold For a four-year inves...      2   \n1197  Auction for Shares In Google's IPO May End Tod...      3   \n1198  Four Firms Buy Intelsat Intelsat Ltd., the pio...      3   \n1199  Auction for Shares in Google's IPO May End Goo...      3   \n\n                                              input_ids  \\\n0     [101, 2813, 2358, 1012, 6468, 15020, 2067, 204...   \n1     [101, 18431, 2571, 3504, 2646, 3293, 13395, 10...   \n2     [101, 3514, 1998, 4610, 6112, 15768, 1005, 176...   \n3     [101, 5712, 9190, 2015, 3514, 14338, 2013, 236...   \n4     [101, 3514, 7597, 2061, 2906, 2000, 2035, 1011...   \n...                                                 ...   \n1195  [101, 8582, 3504, 2000, 8536, 2049, 11550, 105...   \n1196  [101, 9383, 1005, 1055, 3632, 2005, 2751, 2005...   \n1197  [101, 10470, 2005, 6661, 1999, 8224, 1005, 105...   \n1198  [101, 2176, 9786, 4965, 13420, 16846, 13420, 1...   \n1199  [101, 10470, 2005, 6661, 1999, 8224, 1005, 105...   \n\n                                         token_type_ids  \\\n0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n...                                                 ...   \n1195  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1196  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1197  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1198  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1199  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                         attention_mask  \n0     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n2     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n4     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n...                                                 ...  \n1195  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1196  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1197  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1198  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1199  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n\n[1200 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>input_ids</th>\n      <th>token_type_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n      <td>2</td>\n      <td>[101, 2813, 2358, 1012, 6468, 15020, 2067, 204...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n      <td>2</td>\n      <td>[101, 18431, 2571, 3504, 2646, 3293, 13395, 10...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n      <td>2</td>\n      <td>[101, 3514, 1998, 4610, 6112, 15768, 1005, 176...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n      <td>2</td>\n      <td>[101, 5712, 9190, 2015, 3514, 14338, 2013, 236...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Oil prices soar to all-time record, posing new...</td>\n      <td>2</td>\n      <td>[101, 3514, 7597, 2061, 2906, 2000, 2035, 1011...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1195</th>\n      <td>Airline Looks to Delay Its Pension Payments In...</td>\n      <td>2</td>\n      <td>[101, 8582, 3504, 2000, 8536, 2049, 11550, 105...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1196</th>\n      <td>McDonald's Goes for Gold For a four-year inves...</td>\n      <td>2</td>\n      <td>[101, 9383, 1005, 1055, 3632, 2005, 2751, 2005...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1197</th>\n      <td>Auction for Shares In Google's IPO May End Tod...</td>\n      <td>3</td>\n      <td>[101, 10470, 2005, 6661, 1999, 8224, 1005, 105...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1198</th>\n      <td>Four Firms Buy Intelsat Intelsat Ltd., the pio...</td>\n      <td>3</td>\n      <td>[101, 2176, 9786, 4965, 13420, 16846, 13420, 1...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1199</th>\n      <td>Auction for Shares in Google's IPO May End Goo...</td>\n      <td>3</td>\n      <td>[101, 10470, 2005, 6661, 1999, 8224, 1005, 105...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1200 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.to_pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T00:34:29.963850Z",
     "start_time": "2024-01-29T00:34:29.915689Z"
    }
   },
   "id": "b66474eedf17c493",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaypark/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T00:34:29.966559Z",
     "start_time": "2024-01-29T00:34:29.962271Z"
    }
   },
   "id": "a6c7d983ab70cc8a",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS 장치를 지원하도록 build가 되었는가? True\n",
      "MPS 장치가 사용 가능한가? True\n"
     ]
    }
   ],
   "source": [
    "print(f\"MPS 장치를 지원하도록 build가 되었는가? {torch.backends.mps.is_built()}\")\n",
    "print(f\"MPS 장치가 사용 가능한가? {torch.backends.mps.is_available()}\") "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T00:34:29.969029Z",
     "start_time": "2024-01-29T00:34:29.967070Z"
    }
   },
   "id": "980803f4de2666df",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   3%|▎         | 5/150 [00:52<25:22, 10.50s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[41], line 20\u001B[0m\n\u001B[1;32m     18\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     19\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 20\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     22\u001B[0m average_loss \u001B[38;5;241m=\u001B[39m total_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_loader)\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m average loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maverage_loss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    368\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    369\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    370\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    371\u001B[0m             )\n\u001B[0;32m--> 373\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    374\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    376\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:466\u001B[0m, in \u001B[0;36mAdamW.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    462\u001B[0m state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    464\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[1;32m    465\u001B[0m \u001B[38;5;66;03m# In-place operations to update the averages at the same time\u001B[39;00m\n\u001B[0;32m--> 466\u001B[0m exp_avg\u001B[38;5;241m.\u001B[39mmul_(beta1)\u001B[38;5;241m.\u001B[39madd_(grad, alpha\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m-\u001B[39m beta1))\n\u001B[1;32m    467\u001B[0m exp_avg_sq\u001B[38;5;241m.\u001B[39mmul_(beta2)\u001B[38;5;241m.\u001B[39maddcmul_(grad, grad, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m-\u001B[39m beta2)\n\u001B[1;32m    468\u001B[0m denom \u001B[38;5;241m=\u001B[39m exp_avg_sq\u001B[38;5;241m.\u001B[39msqrt()\u001B[38;5;241m.\u001B[39madd_(group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meps\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0 \n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        inputs = {key: batch[key].to(device) for key in batch}\n",
    "        labels = inputs.pop(\"label\")\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} average loss: {average_loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T00:35:24.492885Z",
     "start_time": "2024-01-29T00:34:29.970087Z"
    }
   },
   "id": "5465b6f28fd6b956",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T00:35:24.578357Z",
     "start_time": "2024-01-29T00:35:24.498305Z"
    }
   },
   "id": "4568814be31606a7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
