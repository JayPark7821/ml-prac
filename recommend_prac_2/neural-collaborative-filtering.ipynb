{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-18T03:32:43.239835Z",
     "start_time": "2024-03-18T03:32:40.811425Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MovieLensDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ratings):\n",
    "        super().__init__()\n",
    "        data = ratings.copy().to_numpy()\n",
    "        self.items = data[:, :2].astype(np.int32) - 1\n",
    "        self.targets = self.__preprocess_targets(data[:, 2]).astype(np.float32)\n",
    "        self.field_dims = np.max(self.items, axis=0) + 1\n",
    "        self.user_field_idx = np.array((0,), dtype=np.int64)\n",
    "        self.item_field_idx = np.array((1,), dtype=np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.targets.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.items[idx], self.targets[idx]\n",
    "    \n",
    "    def __preprocess_targets(self, target):\n",
    "        target = target / 5.\n",
    "        return target\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T03:32:43.243789Z",
     "start_time": "2024-03-18T03:32:43.240747Z"
    }
   },
   "id": "38c5c172fac54ba9",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "    def __init__(self, input_dim, embed_dims, dropout, output_layer=True):\n",
    "        super().__init__()\n",
    "        layers = list()\n",
    "        for embed_dim in embed_dims:\n",
    "            layers.append(torch.nn.Linear(input_dim, embed_dim))\n",
    "            layers.append(torch.nn.BatchNorm1d(embed_dim))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            layers.append(torch.nn.Dropout(p=dropout))\n",
    "            input_dim = embed_dim\n",
    "        if output_layer:\n",
    "            layers.append(torch.nn.Linear(input_dim, 1))\n",
    "        self.mlp = torch.nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T03:32:43.246687Z",
     "start_time": "2024-03-18T03:32:43.244329Z"
    }
   },
   "id": "8842e9081ece6be2",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class FeatureEmbedding(torch.nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(sum(field_dims), embed_dim)\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int64)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        return self.embedding(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T03:32:43.249499Z",
     "start_time": "2024-03-18T03:32:43.247178Z"
    }
   },
   "id": "8575b23fd26fbf30",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NeuralCollaborativeFiltering(torch.nn.Module):\n",
    "    def __init__(self, field_dims, user_field_idx, item_field_idx, embed_dim, mlp_dims, dropout):\n",
    "        super().__init__()\n",
    "        self.user_field_idx = user_field_idx\n",
    "        self.item_field_idx = item_field_idx\n",
    "        self.embedding = FeatureEmbedding(field_dims, embed_dim)\n",
    "        self.embed_output_dim = len(field_dims) * embed_dim\n",
    "        self.mlp = MultiLayerPerceptron(self.embed_output_dim, mlp_dims, dropout, output_layer=False)\n",
    "        self.fc = torch.nn.Linear(mlp_dims[-1] + embed_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        user_x = x[:, self.user_field_idx].squeeze(1)\n",
    "        item_x = x[:, self.item_field_idx].squeeze(1)\n",
    "        x = self.mlp(x.view(-1, self.embed_output_dim))\n",
    "        gmf = user_x * item_x\n",
    "        x = torch.cat([gmf, x], dim=1)\n",
    "        x = self.fc(x).squeeze(1)\n",
    "        return torch.sigmoid(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T03:32:43.254042Z",
     "start_time": "2024-03-18T03:32:43.251114Z"
    }
   },
   "id": "877e9e7d9d328d9c",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train(model, optimizer, data_loader, criterion, device, log_interval=100):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    av_loss = []\n",
    "    for i, (fields, target) in enumerate(data_loader):\n",
    "        fields, target = fields.to(device), target.to(device)\n",
    "        y = model(fields)\n",
    "        loss = criterion(y, target.float())\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    loss = total_loss / log_interval\n",
    "    av_loss.append(loss)\n",
    "    total_loss = 0\n",
    "    return np.mean(av_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T03:32:43.256939Z",
     "start_time": "2024-03-18T03:32:43.254689Z"
    }
   },
   "id": "ff209cda8298f265",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def test(model, data_loader, device):\n",
    "    model.eval()\n",
    "    targets, predicts = list(), list()\n",
    "    with torch.no_grad():\n",
    "        for fields, target in data_loader:\n",
    "            fields, target = fields.to(device), target.to(device)\n",
    "            y = model(fields)\n",
    "            targets.extend(target.tolist())\n",
    "            predicts.extend(y.tolist())\n",
    "    return 5. * mean_squared_error(targets, predicts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T03:32:43.259602Z",
     "start_time": "2024-03-18T03:32:43.257483Z"
    }
   },
   "id": "33532c473099889b",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('../data/ml-latest-small/ratings.csv')\n",
    "dataset = MovieLensDataset(ratings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T03:32:43.285679Z",
     "start_time": "2024-03-18T03:32:43.260049Z"
    }
   },
   "id": "9187fee3bdbd1719",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-6\n",
    "batch_size = 2048\n",
    "epochs = 10\n",
    "model_name = \"ncf\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T03:32:43.289069Z",
     "start_time": "2024-03-18T03:32:43.287031Z"
    }
   },
   "id": "767f37ec4e44c55f",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_length = int(len(dataset) * 0.8)\n",
    "valid_length = int(len(dataset) * 0.1)\n",
    "test_length = len(dataset) - train_length - valid_length\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset,\n",
    "                                                                           (train_length, valid_length, test_length))\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=2)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=2)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T03:32:43.297244Z",
     "start_time": "2024-03-18T03:32:43.289602Z"
    }
   },
   "id": "3d0dc5f28cb6d9cb",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = NeuralCollaborativeFiltering(\n",
    "    dataset.field_dims,\n",
    "    embed_dim=64,\n",
    "    mlp_dims=(32, 32),\n",
    "    dropout=0.2,\n",
    "    user_field_idx=dataset.user_field_idx,\n",
    "    item_field_idx=dataset.item_field_idx\n",
    ").to(device)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T03:32:43.757399Z",
     "start_time": "2024-03-18T03:32:43.297666Z"
    }
   },
   "id": "986062f799004f45",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# metric_values = []\n",
    "# loss_values = []\n",
    "# \n",
    "# for epoch_i in tqdm(range(epochs)):\n",
    "#     loss = train(model, optimizer, train_data_loader, criterion, device)\n",
    "#     loss_values.append((epoch_i, loss))\n",
    "#     metric_valid = test(model, valid_data_loader, device)\n",
    "#  \n",
    "#     metric_train = test(model, train_data_loader, device)\n",
    "#     metric_test = test(model, test_data_loader, device)\n",
    "# \n",
    "#     metric_values.append((epoch_i, metric_train, metric_valid, metric_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T03:32:43.759282Z",
     "start_time": "2024-03-18T03:32:43.757973Z"
    }
   },
   "id": "74f42964eb5640b9",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[671, 163949]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset.field_dims)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T03:32:47.957257Z",
     "start_time": "2024-03-18T03:32:47.954191Z"
    }
   },
   "id": "551e2ddd17c9b5a",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/jaypark/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jaypark/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'MovieLensDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m best_valid_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minf\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch_i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m----> 7\u001B[0m     loss \u001B[38;5;241m=\u001B[39m train(model, optimizer, train_data_loader, criterion, device)\n\u001B[1;32m      8\u001B[0m     loss_values\u001B[38;5;241m.\u001B[39mappend((epoch_i, loss))\n\u001B[1;32m     10\u001B[0m     valid_loss \u001B[38;5;241m=\u001B[39m test(model, valid_data_loader, device)\n",
      "Cell \u001B[0;32mIn[6], line 5\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, optimizer, data_loader, criterion, device, log_interval)\u001B[0m\n\u001B[1;32m      3\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m      4\u001B[0m av_loss \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (fields, target) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(data_loader):\n\u001B[1;32m      6\u001B[0m     fields, target \u001B[38;5;241m=\u001B[39m fields\u001B[38;5;241m.\u001B[39mto(device), target\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m      7\u001B[0m     y \u001B[38;5;241m=\u001B[39m model(fields)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:438\u001B[0m, in \u001B[0;36mDataLoader.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator\n\u001B[1;32m    437\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 438\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_iterator()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:386\u001B[0m, in \u001B[0;36mDataLoader._get_iterator\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    384\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    385\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_worker_number_rationality()\n\u001B[0;32m--> 386\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _MultiProcessingDataLoaderIter(\u001B[38;5;28mself\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1039\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter.__init__\u001B[0;34m(self, loader)\u001B[0m\n\u001B[1;32m   1032\u001B[0m w\u001B[38;5;241m.\u001B[39mdaemon \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1033\u001B[0m \u001B[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001B[39;00m\n\u001B[1;32m   1034\u001B[0m \u001B[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001B[39;00m\n\u001B[1;32m   1035\u001B[0m \u001B[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001B[39;00m\n\u001B[1;32m   1036\u001B[0m \u001B[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001B[39;00m\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001B[39;00m\n\u001B[1;32m   1038\u001B[0m \u001B[38;5;66;03m#     AssertionError: can only join a started process.\u001B[39;00m\n\u001B[0;32m-> 1039\u001B[0m w\u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m   1040\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_index_queues\u001B[38;5;241m.\u001B[39mappend(index_queue)\n\u001B[1;32m   1041\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_workers\u001B[38;5;241m.\u001B[39mappend(w)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/multiprocessing/process.py:121\u001B[0m, in \u001B[0;36mBaseProcess.start\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _current_process\u001B[38;5;241m.\u001B[39m_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemon\u001B[39m\u001B[38;5;124m'\u001B[39m), \\\n\u001B[1;32m    119\u001B[0m        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemonic processes are not allowed to have children\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    120\u001B[0m _cleanup()\n\u001B[0;32m--> 121\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_Popen(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sentinel \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen\u001B[38;5;241m.\u001B[39msentinel\n\u001B[1;32m    123\u001B[0m \u001B[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001B[39;00m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;66;03m# reference to the process object (see bpo-30775)\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/multiprocessing/context.py:224\u001B[0m, in \u001B[0;36mProcess._Popen\u001B[0;34m(process_obj)\u001B[0m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[0;32m--> 224\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _default_context\u001B[38;5;241m.\u001B[39mget_context()\u001B[38;5;241m.\u001B[39mProcess\u001B[38;5;241m.\u001B[39m_Popen(process_obj)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/multiprocessing/context.py:288\u001B[0m, in \u001B[0;36mSpawnProcess._Popen\u001B[0;34m(process_obj)\u001B[0m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m    286\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[1;32m    287\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpopen_spawn_posix\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Popen\n\u001B[0;32m--> 288\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Popen(process_obj)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/multiprocessing/popen_spawn_posix.py:32\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, process_obj):\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fds \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 32\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(process_obj)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/multiprocessing/popen_fork.py:19\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturncode \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfinalizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_launch(process_obj)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/multiprocessing/popen_spawn_posix.py:62\u001B[0m, in \u001B[0;36mPopen._launch\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msentinel \u001B[38;5;241m=\u001B[39m parent_r\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(parent_w, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m, closefd\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m---> 62\u001B[0m         f\u001B[38;5;241m.\u001B[39mwrite(fp\u001B[38;5;241m.\u001B[39mgetbuffer())\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     64\u001B[0m     fds_to_close \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "metric_values: list[float] =[]\n",
    "loss_values: list[float] = []\n",
    "model_save_path = \"../data/model.pth\"\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "    loss = train(model, optimizer, train_data_loader, criterion, device)\n",
    "    loss_values.append((epoch_i, loss))\n",
    "    \n",
    "    valid_loss = test(model, valid_data_loader, device)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved at epoch {epoch_i}\")\n",
    "        \n",
    "    metric_valid = test(model, valid_data_loader, device)\n",
    "    metric_train = test(model, train_data_loader, device)\n",
    "    metric_test = test(model, test_data_loader, device)\n",
    "     \n",
    "    metric_values.append((epoch_i, metric_train, metric_valid, metric_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T06:20:57.583653Z",
     "start_time": "2024-03-18T03:35:42.291635Z"
    }
   },
   "id": "cc31f73d06bbb8b1",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a03f92a5c12a8dc7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
